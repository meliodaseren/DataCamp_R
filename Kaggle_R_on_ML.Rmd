---
title: "Kaggle R Tutorial on Machine Learning - R Notebook"
output: html_notebook
---

> This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code.

> Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

> Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

> When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

***

#### Titanic: Machine Learning from Disaster
https://www.kaggle.com/c/titanic/data


#### Install packages
```{r}
install.packages(rpart)
install.packages(rattle)
install.packages(rpart.plot)
install.packages(RColorBrewer)
install.packages(randomForest)
```


## Raising anchor
### Set Sail
```{r}
# Import the training set: train
train_url <- "http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv"
train <- read.csv(train_url)

# Import the testing set: test
test_url <- "http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv"
test <- read.csv(test_url)

# Print train and test to the console
train
test
```


### Understanding your data
#### Rose vs Jack, or Female vs Male
```{r}
# Your train and test set are still loaded
str(train)
str(test)

# Survival rates in absolute numbers
table(train$Survived)

# Survival rates in proportions
prop.table(table(train$Survived))
  
# Two-way comparison: Sex and Survived
table(train$Sex, train$Survived)

# Two-way comparison: row-wise proportions
prop.table(table(train$Sex, train$Survived), 1)
# prop.table(table(train$Sex, train$Survived), 2)
```

#### Does age play a role?
```{r}
# Create the column child, and indicate whether child or no child
train$Child <- NA
train$Child[train$Age < 18] <- 1
train$Child[train$Age >= 18] <- 0

# Two-way comparison
table(train$Child)
prop.table(table(train$Child))
prop.table(table(train$Child, train$Survived), 1)
```


### Making your first predictions
```{r}
# Your train and test set are still loaded in
str(train)
str(test)

# Copy of test
test_one <- test

# Initialize a Survived column to 0
test_one$Survived = 0

# Set Survived to 1 if Sex equals "female"
test_one$Survived[test$Sex == "female"] <- 1

str(test_one)
```


## From icebergs to trees
### Intro to decision trees
Conceptually, the decision tree algorithm starts with all the data at the root node and scans all the variables for the best one to split on.
Once a variable is chosen, you do the split and go down one level (or one node) and repeat.
The final nodes at the bottom of the decision tree are known as terminal nodes, and the majority vote of the observations in that node determine how to predict for new observations that end up in that terminal node.


### Creating first decision tree
Inside [rpart](https://www.rdocumentation.org/packages/rpart/versions/4.1-11), there is the ```rpart()``` function to build your first decision tree. The function takes multiple arguments:
* ```formula```: specifying variable of interest, and the variables used for prediction (e.g. ```formula = Survived ~ Sex + Age```).
* ```data```: The data set to build the decision tree (here ```train```).
* ```method```: Type of prediction you want. We want to predict a categorical variable, so classification: ```method = "class"```.
```{r}
# Load in the R package
library(rpart)

# Build the decision tree
my_tree_two <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
                     data = train,
                     method = "class")

# Visualize the decision tree using plot() and text()
plot(my_tree_two)
text(my_tree_two)

# Load in the packages to build a fancy plot
library(rattle)
library(rpart.plot)
library(RColorBrewer)

# Time to plot your fancy tree
fancyRpartPlot(my_tree_two)

```


### Interpreting your decision tree
Based on your decision tree, variables **"Sex"**, **"Age"**, **"Pclass"**, **"SibSp"**, **"Fare"** play the most important role to determine whether or not a passenger will survive.


### Predict and submit to Kaggle
```{r}
# my_tree_two and test are available in the workspace
my_tree_two
test

# Make predictions on the test set
  # my_tree_two is the tree model you've just built
  # test is the data set to build the preditions for
  # type = "class" specifies that you want to classify observations
my_prediction <- predict(my_tree_two, newdata = test, type = "class")


# Convert predictions to a CSV file with exactly 418 entries and 2 columns PassengerId and Survived

# Finish the data.frame() call
my_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)

# Use nrow() on my_solution
nrow(my_solution)

# Finish the write.csv() call
write.csv(my_solution, file = "my_solution1.csv", row.names = FALSE)
```


### Overfitting, the iceberg of decision trees
Maybe we can improve even more by making a more complex model?
In rpart, the amount of detail is defined by two parameters:
* ```cp``` determines when the splitting up of the decision tree stops.
* ```minsplit``` determines the minimum amount of observations in a leaf of the tree.

In the ```super_model``` on the right, ```cp = 0``` (no stopping of splits) and ```minsplit = 2``` (smallest leaf possible).
This will create the best model! Or not? Check out the resulting plot with:

```fancyRpartPlot(super_model)```

Looking complex, but using this model to make predictions won't give you a good score on Kaggle.
Why? Because you created very specific rules based on the data in the training set.
These very detailed rules are only relevant for the training set but cannot be generalized to unknown sets.
You overfitted your tree. Always be aware of this danger!

```{r}
# Your train and test set are still loaded in

# Change this command
my_tree_three <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
                       data = train, method = "class",
                       control = rpart.control(minsplit = 50, cp = 0))

# Visualize my_tree_three
fancyRpartPlot(my_tree_three)
```


### Re-engineering our Titanic data set
Data Science is an art that benefits from a human element.
Enter feature engineering: creatively engineering your own features by combining the different existing variables.

While feature engineering is a discipline in itself, too broad to be covered here in detail, let's have have a look at a simple example and create a new predictive attribute: ```family_size```.

A valid assumption is that larger families need more time to get together on a sinking ship, and hence have less chance of surviving.
* ```SibSp```	# of siblings / spouses aboard the Titanic
* ```Parch```	# of parents / children aboard the Titanic
Family size is determined by the variables ```SibSp``` and ```Parch```, which indicate the number of family members a certain passenger is traveling with.
So when doing feature engineering, you add a new variable ```family_size```, which is the sum of ```SibSp``` and ```Parch``` plus one (the observation itself), to the test and train set.

```{r}
# train and test are available

# Create train_two and new variable
train_two <- train
train_two$family_size <- train_two$SibSp + train_two$Parch + 1

# Finish the command
my_tree_four <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + family_size,
                      data = train_two, method = "class")

# Visualize your new decision tree
fancyRpartPlot(my_tree_four)
```


### Passenger Title and survival rate
**This part only work on [DataCamp](https://campus.datacamp.com/courses/kaggle-r-tutorial-on-machine-learning/chapter-2-from-icebergs-to-trees?ex=7)**

Access to a new train and test set named ```train_new``` and ```test_new```.
These data sets contain a new column with the name ```Title``` (referring to Miss, Mr, etc.).
```Title``` is another example of feature engineering: it's a new variable that possibly improves the model.

```{r}
# train_new and test_new are available in the workspace

str(train_new)
# 'data.frame':	891 obs. of  13 variables:
# $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
# $ Survived   : int  0 1 1 1 0 0 0 0 1 1 ...
# $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
# $ Name       : chr  "Braund, Mr. Owen Harris" "Cumings, Mrs. John Bradley (Florence Briggs Thayer)" "Heikkinen, Miss. Laina" "Futrelle, Mrs. Jacques Heath (Lily May Peel)" ...
# $ Sex        : Factor w/ 2 levels "female","male": 2 1 1 1 2 2 2 2 1 1 ...
# $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
# $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
# $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
# $ Ticket     : Factor w/ 929 levels "110152","110413",..: 524 597 670 50 473 276 86 396 345 133 ...
# $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
# $ Cabin      : Factor w/ 187 levels "","A10","A14",..: 1 83 1 57 1 1 131 1 1 1 ...
# $ Embarked   : Factor w/ 4 levels "","C","Q","S": 4 2 4 4 4 3 4 4 4 2 ...
# $ Title      : Factor w/ 11 levels "Col","Dr","Lady",..: 7 8 5 8 7 7 7 4 8 8 ...

str(test_new)
# 'data.frame':	418 obs. of  13 variables:
# $ PassengerId: int  892 893 894 895 896 897 898 899 900 901 ...
# $ Survived   : int  NA NA NA NA NA NA NA NA NA NA ...
# $ Pclass     : int  3 3 2 3 3 3 3 2 3 3 ...
# $ Name       : chr  "Kelly, Mr. James" "Wilkes, Mrs. James (Ellen Needs)" "Myles, Mr. Thomas Francis" "Wirz, Mr. Albert" ...
# $ Sex        : Factor w/ 2 levels "female","male": 2 1 2 2 1 2 1 2 1 2 ...
# $ Age        : num  34.5 47 62 27 22 14 30 26 18 21 ...
# $ SibSp      : int  0 1 0 0 1 0 0 1 0 2 ...
# $ Parch      : int  0 0 0 0 1 0 0 1 0 0 ...
# $ Ticket     : Factor w/ 929 levels "110152","110413",..: 781 841 726 776 252 869 787 159 745 520 # ...
# $ Fare       : num  7.83 7 9.69 8.66 12.29 ...
# $ Cabin      : Factor w/ 187 levels "","A10","A14",..: 1 1 1 1 1 1 1 1 1 1 ...
# $ Embarked   : Factor w/ 4 levels "","C","Q","S": 3 4 3 4 4 4 3 4 2 4 ...
# $ Title      : Factor w/ 11 levels "Col","Dr","Lady",..: 7 8 7 7 8 7 5 7 8 7 ...

# Finish the command
my_tree_five <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title,
                      data = train_new, method = "class")

# Visualize my_tree_five
fancyRpartPlot(my_tree_five)

# Make prediction
my_prediction <- predict(my_tree_five, test_new, type = "class")

# Make results ready for submission
my_solution <- data.frame(PassengerId = test_new$PassengerId, Survived = my_prediction)
write.csv(my_solution, file = "my_solution2.csv", row.names = FALSE)
```


## Improving Your Predictions Through Random Forests
### What is a Random Forest
In layman's terms, the Random Forest technique **handles the overfitting problem** you faced with decision trees.
It grows multiple (very deep) classification trees using the training set.
At the time of prediction, each tree is used to come up with a prediction and every outcome is counted as a vote.
For example, if you have trained 3 trees with 2 saying a passenger in the test set will survive and 1 says he will not, the passenger will be classified as a survivor.
This approach of overtraining trees, but having **the majority's vote** count as the actual classification decision, avoids overfitting.

Before starting with the actual analysis, you first need to meet one big condition of Random Forests: **no missing values in your data frame**. Let's get to work.

```{r}
# All data, both training and test set
# http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/all_data.RData
all_data

# Passenger on row 62 and 830 do not have a value for embarkment.
# Since many passengers embarked at Southampton, we give them the value S.
all_data$Embarked[c(62, 830)] <- "S"

# Factorize embarkment codes.
all_data$Embarked <- factor(all_data$Embarked)

# Passenger on row 1044 has an NA Fare value. Let's replace it with the median fare value.
all_data$Fare[1044] <- median(all_data$Fare, na.rm = TRUE)

# How to fill in missing Age values?
# We make a prediction of a passengers Age using the other variables and a decision tree model.
# This time you give method = "anova" since you are predicting a continuous variable.
library(rpart)
predicted_age <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + family_size,
                       data = all_data[!is.na(all_data$Age),], method = "anova")
all_data$Age[is.na(all_data$Age)] <- predict(predicted_age, all_data[is.na(all_data$Age),])

# Split the data back into a train set and a test set
train <- all_data[1:891,]
test <- all_data[892:1309,]
```


### A Random Forest analysis in R
https://www.rdocumentation.org/packages/randomForest/versions/4.6-12
For a Random Forest analysis in R you make use of the ```randomForest()``` function in the randomForest package.
You call the function in a similar way as ```rpart()```:
* First your provide the ```formula```. There is no argument ```class``` here to inform the function you're dealing with predicting a categorical variable, so you need to turn Survived into a factor with two levels: ```as.factor(Survived) ~ Pclass + Sex + Age```
* The ```data``` argument takes the ```train``` data frame.
* When you put the ```importance``` argument to ```TRUE``` you can inspect variable importance.
* The ```ntree``` argument specifies the number of trees to grow. Limit these when having only limited computational power at your disposal.
To end, since Random Forest uses randomization, you set a seed like this ```set.seed(111)``` to assure reproducibility of your results.
Once the model is constructed, you can use the prediction function ```predict()```.

```{r}
# train and test are available in the workspace
str(train)
str(test)

# Load in the package
library(randomForest)

# Set seed for reproducibility
set.seed(111)

# Apply the Random Forest Algorithm
my_forest <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title,
                          data = train, importance = TRUE, ntree = 1000)

# Make your prediction using the test set
my_prediction <- predict(my_forest, test)

# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions
my_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)

# Write your solution away to a csv file with the name my_solution.csv
write.csv(my_solution, file = "my_solution3.csv", row.names = FALSE)
```


### Important variables
Your Random Forest object ```my_forest``` is still loaded in.
Remember you set ```importance = TRUE```? Now you can see what variables are important using
```{r}
varImpPlot(my_forest)
```

Based on the two plots, variable "Title" has the highest impact on the model.
